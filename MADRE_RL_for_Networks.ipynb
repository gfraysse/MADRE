{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MADRE - 12/01/2026\n",
        "\n",
        "DQN agent for the training of Atari games and Classic Control environment\n",
        "\n",
        "Code mostly taken from Keras documentation at\n",
        "https://keras.io/examples/rl/deep_q_network_breakout/"
      ],
      "metadata": {
        "id": "6f06hAIbgbqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies to Render Gym Environment"
      ],
      "metadata": {
        "id": "3y3OCc4LHVRj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eY-swb0vHG9-"
      },
      "outputs": [],
      "source": [
        "# Run these cells ASAP since it can take up to 30 seconds\n",
        "%%capture\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb python-opengl ffmpeg\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyglet==1.3.2 gymnasium==1.2.2 pyvirtualdisplay==3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzllNAuKRcj3",
        "outputId": "e03a9e4b-7b30-447c-f9bb-965ddb746021"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyglet==1.3.2 in /usr/local/lib/python3.12/dist-packages (1.3.2)\n",
            "Requirement already satisfied: gymnasium==1.2.2 in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: pyvirtualdisplay==3.0 in /usr/local/lib/python3.12/dist-packages (3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from pyglet==1.3.2) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==1.2.2) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==1.2.2) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==1.2.2) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium==1.2.2) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install tensorflow==2.19.0 gast==0.7"
      ],
      "metadata": {
        "id": "LRbsjfKcPZFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce94349-6baf-489b-d21e-337bfae5656a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.19.0\n",
            "  Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: gast==0.7 in /usr/local/lib/python3.12/dist-packages (0.7.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.9.23)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.19.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19.0) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19.0) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (645.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.0/645.0 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.19.1\n",
            "    Uninstalling tensorflow-2.19.1:\n",
            "      Successfully uninstalled tensorflow-2.19.1\n",
            "Successfully installed tensorflow-2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ale-py gymnasium[other]\n",
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "xK_FNuH-RUBK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027ab398-2518-4561-ced4-9c87a6f0eec3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.12/dist-packages (0.11.2)\n",
            "Requirement already satisfied: gymnasium[other] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.12/dist-packages (from ale-py) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[other]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[other]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[other]) (0.0.4)\n",
            "Requirement already satisfied: moviepy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[other]) (1.0.3)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[other]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[other]) (4.12.0.88)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.12/dist-packages (from gymnasium[other]) (0.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[other]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[other]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[other]) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[other]) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[other]) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[other]) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[other]) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->gymnasium[other]) (2.9.0.post0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[other]) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[other]) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[other]) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[other]) (0.1.12)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[other]) (2.37.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy>=1.0.0->gymnasium[other]) (0.6.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn>=0.13->gymnasium[other]) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn>=0.13->gymnasium[other]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn>=0.13->gymnasium[other]) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[other]) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[other]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[other]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[other]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[other]) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium\n",
        "\n",
        "from gymnasium import logger as gymlogger\n",
        "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation #Monitor\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "import ale_py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "import glob\n",
        "import io\n",
        "import base64"
      ],
      "metadata": {
        "id": "HBkAENQYIaKp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# virtual display for notebookfrom IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ0PUv4mfUpj",
        "outputId": "8a94b4a7-d97d-4b42-81ec-9fe530d3da13"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7c99d1f76f90>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check that there is a GPU avaiable"
      ],
      "metadata": {
        "id": "0TlFScs6HgHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_list = tf.config.experimental.list_physical_devices('GPU')\n",
        "print('Number of GPUS available is {}'.format(len(gpu_list)))"
      ],
      "metadata": {
        "id": "iuYJDMT5Hj0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2299dfc-3845-47f1-9d6a-74d6531b5c14"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of GPUS available is 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions to visualize the performance of the agent"
      ],
      "metadata": {
        "id": "fmb043knKlw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_video():\n",
        "  \"\"\"Enables video recording of gym environment and shows it.\"\"\"\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Video not found\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "metadata": {
        "id": "8SJkiuHNKpPg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Environment and Build Dueling DQN Agent"
      ],
      "metadata": {
        "id": "7yhD05g6J7Mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gymnasium.register_envs(ale_py)"
      ],
      "metadata": {
        "id": "5ntuA4zHO_V3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selection of the environment\n",
        "#Full list of Atari games environment at https://ale.farama.org/environments/\n",
        "environment_name = \"Pong\" # @param [\"Pong\", \"Breakout\", \"MontezumaRevenge\",\"MsPacman\",\"SpaceInvaders\"]\n",
        "\n",
        "save_model = True # @param {type:\"boolean\"}\n",
        "\n",
        "# Hyperparameters.\n",
        "num_episodes = 10000 # @param {type:\"integer\"}\n",
        "epsilon = 1.0 # @param {type:\"number\"}\n",
        "batch_size = 32 # @param {type:\"integer\"}\n",
        "discount = 0.99 # @param {type:\"number\"}\n",
        "# How often to update the target network\n",
        "update_target_network = 10000 # @param {type:\"integer\"}\n",
        "# Train the model after 4 actions\n",
        "update_after_actions = 4 # @param {type:\"integer\"}\n",
        "\n",
        "seed = 42\n"
      ],
      "metadata": {
        "id": "TyP2xYfuKaGo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(environment_name, render='rgb_array'):\n",
        "  # Load gym environment and get action and state spaces\n",
        "  #env = gymnasium.make(\"ALE/\"+environment_name+\"-v5\", render_mode=\"rgb_array\")\n",
        "  env = gymnasium.make(environment_name+\"NoFrameskip-v4\", render_mode=render)\n",
        "\n",
        "  env = AtariPreprocessing(env)\n",
        "  # Stack four frames\n",
        "  env = FrameStackObservation(env, 4)\n",
        "\n",
        "  # Record a video every num_episodes steps\n",
        "  trigger = lambda t: t % 100 == 0\n",
        "  env = RecordVideo(env, video_folder=\"./video\", episode_trigger=trigger, name_prefix=environment_name, disable_logger=True)\n",
        "  return env"
      ],
      "metadata": {
        "id": "dGzoHLtMJ1NO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_env(environment_name)\n",
        "num_state_feats = env.observation_space.shape\n",
        "num_actions = env.action_space.n\n",
        "max_observation_values = env.observation_space.high\n",
        "print('Number of state features: {}'.format(num_state_feats))\n",
        "print('Number of possible actions: {}'.format(num_actions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "306hxFRAjvar",
        "outputId": "dc95ee54-74e6-43da-9903-bac4df7f28ea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of state features: (4, 84, 84)\n",
            "Number of possible actions: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN model"
      ],
      "metadata": {
        "id": "x_nHPN4nf_Pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_q_model():\n",
        "    # Network defined by the Deepmind paper\n",
        "    return keras.Sequential(\n",
        "        [\n",
        "            layers.Lambda(\n",
        "                lambda tensor: keras.ops.transpose(tensor, [0, 2, 3, 1]),\n",
        "                output_shape=(84, 84, 4),\n",
        "                input_shape=(4, 84, 84),\n",
        "            ),\n",
        "            # Convolutions on the frames on the screen\n",
        "            layers.Conv2D(32, 8, strides=4, activation=\"relu\", input_shape=(4, 84, 84)),\n",
        "            layers.Conv2D(64, 4, strides=2, activation=\"relu\"),\n",
        "            layers.Conv2D(64, 3, strides=1, activation=\"relu\"),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(512, activation=\"relu\"),\n",
        "            layers.Dense(num_actions, activation=\"linear\"),\n",
        "        ]\n",
        "    )"
      ],
      "metadata": {
        "id": "MkfBRt-AKBZn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Helper Functions"
      ],
      "metadata": {
        "id": "BALsJS3KKFbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_epsilon_greedy_action(state, epsilon):\n",
        "  \"\"\"Take random action with probability epsilon, else take best action.\"\"\"\n",
        "  result = np.random.uniform()\n",
        "  if result < epsilon:\n",
        "    return env.action_space.sample() # Random action.\n",
        "  else:\n",
        "    qs = main_nn(state).numpy()\n",
        "    return np.argmax(qs) # Greedy action for state."
      ],
      "metadata": {
        "id": "l5RSPRFmKI7E"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up Function to Perform a Training Step"
      ],
      "metadata": {
        "id": "ybFUgowIKPpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(states, actions, rewards, next_states, dones):\n",
        "  \"\"\"Perform a training iteration on a batch of data.\"\"\"\n",
        "  next_qs_main = main_nn(next_states)\n",
        "  next_qs_argmax = tf.argmax(next_qs_main, axis=-1)\n",
        "  next_action_mask = tf.one_hot(next_qs_argmax, num_actions)\n",
        "  next_qs_target = target_nn(next_states)\n",
        "  masked_next_qs = tf.reduce_sum(next_action_mask * next_qs_target, axis=-1)\n",
        "  target = rewards + (1. - dones) * discount * masked_next_qs\n",
        "  with tf.GradientTape() as tape:\n",
        "    qs = main_nn(states)\n",
        "    action_mask = tf.one_hot(actions, num_actions)\n",
        "    masked_qs = tf.reduce_sum(action_mask * qs, axis=-1)\n",
        "    loss = loss_fn(target, masked_qs)\n",
        "  grads = tape.gradient(loss, main_nn.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, main_nn.trainable_variables))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "ppVTxe2yKO8a"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start running the DQN algorithm and see how the algorithm learns."
      ],
      "metadata": {
        "id": "bzerCzNzKbLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experience replay buffers\n",
        "action_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []\n",
        "episode_reward_history = []\n",
        "# Maximum replay length\n",
        "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
        "max_memory_length = 100000"
      ],
      "metadata": {
        "id": "MpGoWMafI8T0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_nn = create_q_model()\n",
        "target_nn = create_q_model()\n",
        "\n",
        "# Loss function and optimizer.\n",
        "loss_fn = tf.keras.losses.Huber()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, clipnorm=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N8GrkbhkfLp",
        "outputId": "4d450e2d-91c2-4bf6-b6a3-0a37bdafee7d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/lambda_layer.py:65: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training.\n",
        "cur_frame, last_100_ep_rewards = 0, []\n",
        "for episode in range(num_episodes + 1):\n",
        "  state, _info = env.reset(seed=seed)\n",
        "  ep_reward, done = 0, False\n",
        "  while not done:\n",
        "    state_in = np.expand_dims(np.array(state) / 255., axis=0)\n",
        "    action = select_epsilon_greedy_action(state_in, epsilon)\n",
        "    next_state, reward, done, _truncated, info = env.step(action)\n",
        "    ep_reward += reward\n",
        "    reward = np.sign(reward)\n",
        "\n",
        "    # Save to experience replay.\n",
        "    action_history.append(action)\n",
        "    state_history.append(state)\n",
        "    state_next_history.append(next_state)\n",
        "    done_history.append(done)\n",
        "    rewards_history.append(reward)\n",
        "\n",
        "    state = next_state\n",
        "    cur_frame += 1\n",
        "    if epsilon > 0.01:\n",
        "      epsilon -= 1.1e-6\n",
        "\n",
        "    if cur_frame % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "      indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
        "      # Using list comprehension to sample from replay buffer\n",
        "      states = np.array([state_history[i] for i in indices])\n",
        "      next_states = np.array([state_next_history[i] for i in indices])\n",
        "      rewards = [rewards_history[i] for i in indices]\n",
        "      actions = [action_history[i] for i in indices]\n",
        "      dones = keras.ops.convert_to_tensor(\n",
        "        [float(done_history[i]) for i in indices]\n",
        "      )\n",
        "\n",
        "      states = states / 255.\n",
        "      next_states = next_states / 255.\n",
        "      loss = train_step(states, actions, rewards, next_states, dones)\n",
        "\n",
        "    # Copy main_nn weights to target_nn.\n",
        "    if cur_frame % update_target_network == 0:\n",
        "      target_nn.set_weights(main_nn.get_weights())\n",
        "\n",
        "    # Limit the state and reward history\n",
        "    if len(rewards_history) > max_memory_length:\n",
        "      del rewards_history[:1]\n",
        "      del state_history[:1]\n",
        "      del state_next_history[:1]\n",
        "      del action_history[:1]\n",
        "      del done_history[:1]\n",
        "\n",
        "  if len(last_100_ep_rewards) == 100:\n",
        "    last_100_ep_rewards = last_100_ep_rewards[1:]\n",
        "  last_100_ep_rewards.append(ep_reward)\n",
        "\n",
        "  if episode % 10 == 0:\n",
        "    print(f'Episode: {episode}/{num_episodes}, Epsilon: {epsilon:.3f}, '\\\n",
        "          f'Loss: {loss:.4f}, Return: {np.mean(last_100_ep_rewards):.2f}')\n",
        "\n",
        "  if episode > 0 and episode % 1000 == 0 and save_model:\n",
        "      model_path = environment_name + \".keras\"\n",
        "      main_nn.save(model_path)\n",
        "      #print(f\"model saved to {model_path}\")\n",
        "\n",
        "env.close()\n",
        "show_video()"
      ],
      "metadata": {
        "id": "a9y2zn3bKh_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21cacde1-387f-429d-d994-66b0d236ddce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0/10000, Epsilon: 0.999, Loss: 0.0003, Return: -21.00\n",
            "Episode: 10/10000, Epsilon: 0.989, Loss: 0.0009, Return: -20.55\n",
            "Episode: 20/10000, Epsilon: 0.980, Loss: 0.0002, Return: -20.52\n",
            "Episode: 30/10000, Epsilon: 0.969, Loss: 0.0003, Return: -20.32\n",
            "Episode: 40/10000, Epsilon: 0.959, Loss: 0.0011, Return: -20.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display Result of Trained DQN Agent on Pong Environment"
      ],
      "metadata": {
        "id": "Fu95eWSIKscL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gymnasium.make('PongNoFrameskip-v4')\n",
        "env = AtariPreprocessing(env,\n",
        "                         grayscale_obs=True,\n",
        "                         scale_obs=True,\n",
        "                         terminal_on_life_loss=False)\n",
        "env = wrap_env(FrameStack(env, num_stack=4))\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "ep_rew = 0\n",
        "while not done:\n",
        "  env.render()\n",
        "  state = np.array(state)\n",
        "  state = np.expand_dims(state, axis=0)\n",
        "  action = select_epsilon_greedy_action(state, epsilon=0.01)\n",
        "  state, reward, done, info = env.step(action)\n",
        "  ep_rew += reward\n",
        "print(f'Total Return: {ep_rew}')\n",
        "env.close()\n",
        "show_video()"
      ],
      "metadata": {
        "id": "Pn8dLc8LKvBY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}